write.csv(df_final, file = outfile)
}
# py.path = "C:/Program Files/QGIS 2.18/bin/python.exe"
py.path = "E:/Users/engelstad/VisTrails_SAHM/Python27_64/python.exe"
gdal.calc.path = "E:/Users/engelstad/VisTrails_SAHM/Python27_64/Lib/site-packages/osgeo/gdal_calc.py"
in.rast = "E:/Users/engelstad/USGS/data/zonal stats shapefiles/ensemble_1st.tif"
max.val = 3
pts = "E:/Users/engelstad/USGS/data/20190723/fountaingrass/CovariateCorrelationOutputMDS_KDE_initial.csv"
species.name = "fountaingrass"
outfile = "E:/Users/engelstad/USGS/data/zonal stats shapefiles/test.csv"
summary.stats(py.path = py.path,
gdal.calc.path = gdal.calc.path,
in.rast = in.rast,
max.val = max.val,
species.name = species.name,
pts = pts,
outfile = outfile)
# Since this is hard-coded, don't move this folder!
# shp.list <- list.files('//igskbacbfs1/InvasivesSynologyData/Projects/NPS/PostProcessingScripts/zonal_stats_shapefiles',
#                        pattern = '(.*zonal.*shp$)',  recursive = T, full.names = T)
shp.list <- list.files('E:/Users/engelstad/USGS/data/zonal stats shapefiles',
pattern = '(.*zonal.*shp$)',  recursive = T, full.names = T)
shp.list
# Since this is hard-coded, don't move this folder!
# shp.list <- list.files('//igskbacbfs1/InvasivesSynologyData/Projects/NPS/PostProcessingScripts/zonal_stats_shapefiles',
#                        pattern = '(.*zonal.*shp$)',  recursive = T, full.names = T)
shp.list <- list.files('E:/Users/engelstad/USGS/data/zonal stats shapefiles',
pattern = '(.*zonal.*shp$)',  recursive = T, full.names = T)
shp.list
shp.list[4]
s=shp.list[3]
s
# Here, the ensemble_1st from gdal_calc of 1 where A >= n and NoDataValue=0
shp.t <- readOGR(s, stringsAsFactors = F, GDAL1_integer64_policy = T, verbose = F)
# Transform shp to raster's crs
shp.n = spTransform(shp.t, CRSobj = crs(r.l))
# Generate raster metadata.
r.l <- raster(out.rast)
r.i <- GDALinfo(out.rast, silent = T)
# Create threshold raster
out.rast = paste0(tools::file_path_sans_ext(in.rast), '_gte.tif', sep = '')
# Generate raster metadata.
r.l <- raster(out.rast)
r.i <- GDALinfo(out.rast, silent = T)
ps.sq = r.i[6]*r.i[7]
# Read in MDS file to use for point generation.
csv = read.csv(pts, header = T, stringsAsFactors = F)
csv = csv[3:nrow(csv), 1:3]
csv = as.data.frame(sapply(csv[csv$responseBinary ==1, ], as.numeric))
# This line assumes that the input raster and the points are in the same projection.
sp.df = SpatialPointsDataFrame(coords = csv, data = csv, proj4string = crs(r.l))
# Here, the ensemble_1st from gdal_calc of 1 where A >= n and NoDataValue=0
shp.t <- readOGR(s, stringsAsFactors = F, GDAL1_integer64_policy = T, verbose = F)
# Transform shp to raster's crs
shp.n = spTransform(shp.t, CRSobj = crs(r.l))
shp.n[[66]]
shp.n[66,]
summary.stats = function(py.path, gdal.calc.path, in.rast, max.val, species.name, pts, outfile){
# Create threshold raster
out.rast = paste0(tools::file_path_sans_ext(in.rast), '_gte.tif', sep = '')
command = paste0(shQuote(py.path), ' ', shQuote(gdal.calc.path),
' -A ', shQuote(in.rast),
' --type=Int16 --calc="1*(logical_and(A >= ',ceiling(max.val/2),
', A < 100))" --co=COMPRESS=LZW --co=TILED=YES --NoDataValue=0.0 --overwrite --outfile=',
shQuote(out.rast),
sep = '')
system(command)
# Since this is hard-coded, don't move this folder!
# shp.list <- list.files('//igskbacbfs1/InvasivesSynologyData/Projects/NPS/PostProcessingScripts/zonal_stats_shapefiles',
#                        pattern = '(.*zonal.*shp$)',  recursive = T, full.names = T)
shp.list <- list.files('E:/Users/engelstad/USGS/data/zonal stats shapefiles',
pattern = '(.*zonal.*shp$)',  recursive = T, full.names = T)
# Generate raster metadata.
r.l <- raster(out.rast)
r.i <- GDALinfo(out.rast, silent = T)
ps.sq = r.i[6]*r.i[7]
# Read in MDS file to use for point generation.
csv = read.csv(pts, header = T, stringsAsFactors = F)
csv = csv[3:nrow(csv), 1:3]
csv = as.data.frame(sapply(csv[csv$responseBinary ==1, ], as.numeric))
# This line assumes that the input raster and the points are in the same projection.
sp.df = SpatialPointsDataFrame(coords = csv, data = csv, proj4string = crs(r.l))
# This variable will house the df objects from each successive shapefile in the loop
df.list <- list()
for(s in shp.list){
# Here, the ensemble_1st from gdal_calc of 1 where A >= n and NoDataValue=0
shp.t <- readOGR(s, stringsAsFactors = F, GDAL1_integer64_policy = T, verbose = F)
# Transform shp to raster's crs
shp.n = spTransform(shp.t, CRSobj = crs(r.l))
# shp.n = shp.n[1:10,]
#Pull zonal summary of threshold raster - this is the part that takes the longest
result = zonal.stats(x = shp.n, y = r.l, stat = sum) #results match zonal stats from QGIS
result = ifelse(is.na(result), 0, result)
# convert results into data frame. readOGR orders things in a weird way but this method syncs everything up
df = data.frame(Species = species.name
, Unit = shp.n$Unit
, count_n = result)
# Determine rasters size and add square km area to data frame (assumes projection is using meters)
df = as.data.frame(df) %>%
mutate(est_suit_area_km2 = (count_n*(ps.sq/1000000)),
total_area_km2 = area(shp.n)/1000000,
est_suit_area_acre = round(est_suit_area_km2*247.11),
total_area_acre = round(total_area_km2*247.11),
perc_suit = round(est_suit_area_km2/total_area_km2*100))
# Figure out which parks already have known occurrences. Then count them and add them to the 'df' object
pts_in_poly <- spatialEco::point.in.poly(x = sp.df, y = shp.n, duplicate = F)
pts_in_poly <- pts_in_poly@data[!is.na(pts_in_poly@data$pid1),]
pts_in_poly <- pts_in_poly %>%
group_by(pid1) %>%
count() %>%
mutate(Known_Presences = paste0("Yes (", n, ")", sep='')) %>%
rename(Unit = pid1) %>%
droplevels
# Determine the distance from polygon vertices to nearest occurrence point (not a perfect solution but pretty speedy)
dist = as.data.frame(gDistance(spgeom1 = shp.n, spgeom2 = sp.df, byid = T))
colnames(dist) <- shp.n@data$Unit
dist_t = as.data.frame(apply(dist, 2, min)) %>%
mutate(Unit = names(dist)) %>%
rename(min_dist_m = 1) %>%
select(Unit, min_dist_m) %>%
mutate(min_dist_miles = round(min_dist_m*0.0006213712)) #convert meters to miles
#Add everything to the df object
df_add = df %>%
left_join(y = pts_in_poly, by = "Unit") %>%
inner_join(y = dist_t, by = "Unit") %>%
mutate(Known_Presences = ifelse(is.na(Known_Presences), 0, Known_Presences),
perc_suit = ifelse(perc_suit > 100, 1, perc_suit/100),
est_suit_area_acre = ifelse(perc_suit == 100, total_area_acre, est_suit_area_acre),
Admin = unique(shp.n$Admin)) %>%
select(Species, Admin, Unit, est_suit_area_acre, total_area_acre, perc_suit, Known_Presences, min_dist_miles) %>%
arrange(desc(perc_suit))
df.list[[paste0(unique(df_add$Admin))]] <- df_add
}
# Flatten the data frames and write them to the outfile.
df_final = bind_rows(df.list)
write.csv(df_final, file = outfile)
}
py.path = "E:/Users/engelstad/VisTrails_SAHM/Python27_64/python.exe"
gdal.calc.path = "E:/Users/engelstad/VisTrails_SAHM/Python27_64/Lib/site-packages/osgeo/gdal_calc.py"
in.rast = "E:/Users/engelstad/USGS/data/zonal stats shapefiles/ensemble_1st.tif"
max.val = 3
pts = "E:/Users/engelstad/USGS/data/20190723/fountaingrass/CovariateCorrelationOutputMDS_KDE_initial.csv"
species.name = "fountaingrass"
outfile = "E:/Users/engelstad/USGS/data/zonal stats shapefiles/test.csv"
summary.stats(py.path = py.path,
gdal.calc.path = gdal.calc.path,
in.rast = in.rast,
max.val = max.val,
species.name = species.name,
pts = pts,
outfile = outfile)
# Create threshold raster
out.rast = paste0(tools::file_path_sans_ext(in.rast), '_gte.tif', sep = '')
command = paste0(shQuote(py.path), ' ', shQuote(gdal.calc.path),
' -A ', shQuote(in.rast),
' --type=Int16 --calc="1*(logical_and(A >= ',ceiling(max.val/2),
', A < 100))" --co=COMPRESS=LZW --co=TILED=YES --NoDataValue=0.0 --overwrite --outfile=',
shQuote(out.rast),
sep = '')
# Since this is hard-coded, don't move this folder!
# shp.list <- list.files('//igskbacbfs1/InvasivesSynologyData/Projects/NPS/PostProcessingScripts/zonal_stats_shapefiles',
#                        pattern = '(.*zonal.*shp$)',  recursive = T, full.names = T)
shp.list <- list.files('E:/Users/engelstad/USGS/data/zonal stats shapefiles',
pattern = '(.*zonal.*shp$)',  recursive = T, full.names = T)
# Generate raster metadata.
r.l <- raster(out.rast)
r.i <- GDALinfo(out.rast, silent = T)
ps.sq = r.i[6]*r.i[7]
# Read in MDS file to use for point generation.
csv = read.csv(pts, header = T, stringsAsFactors = F)
csv = csv[3:nrow(csv), 1:3]
csv = as.data.frame(sapply(csv[csv$responseBinary ==1, ], as.numeric))
# This line assumes that the input raster and the points are in the same projection.
sp.df = SpatialPointsDataFrame(coords = csv, data = csv, proj4string = crs(r.l))
# This variable will house the df objects from each successive shapefile in the loop
df.list <- list()
for(s in shp.list[3]){
# Here, the ensemble_1st from gdal_calc of 1 where A >= n and NoDataValue=0
shp.t <- readOGR(s, stringsAsFactors = F, GDAL1_integer64_policy = T, verbose = F)
# Transform shp to raster's crs
shp.n = spTransform(shp.t, CRSobj = crs(r.l))
# shp.n = shp.n[1:10,]
#Pull zonal summary of threshold raster - this is the part that takes the longest
result = zonal.stats(x = shp.n, y = r.l, stat = sum) #results match zonal stats from QGIS
result = ifelse(is.na(result), 0, result)
# convert results into data frame. readOGR orders things in a weird way but this method syncs everything up
df = data.frame(Species = species.name
, Unit = shp.n$Unit
, count_n = result)
# Determine rasters size and add square km area to data frame (assumes projection is using meters)
df = as.data.frame(df) %>%
mutate(est_suit_area_km2 = (count_n*(ps.sq/1000000)),
total_area_km2 = area(shp.n)/1000000,
est_suit_area_acre = round(est_suit_area_km2*247.11),
total_area_acre = round(total_area_km2*247.11),
perc_suit = round(est_suit_area_km2/total_area_km2*100))
# Figure out which parks already have known occurrences. Then count them and add them to the 'df' object
pts_in_poly <- spatialEco::point.in.poly(x = sp.df, y = shp.n, duplicate = F)
pts_in_poly <- pts_in_poly@data[!is.na(pts_in_poly@data$pid1),]
pts_in_poly <- pts_in_poly %>%
group_by(pid1) %>%
count() %>%
mutate(Known_Presences = paste0("Yes (", n, ")", sep='')) %>%
rename(Unit = pid1) %>%
droplevels
# Determine the distance from polygon vertices to nearest occurrence point (not a perfect solution but pretty speedy)
dist = as.data.frame(gDistance(spgeom1 = shp.n, spgeom2 = sp.df, byid = T))
colnames(dist) <- shp.n@data$Unit
dist_t = as.data.frame(apply(dist, 2, min)) %>%
mutate(Unit = names(dist)) %>%
rename(min_dist_m = 1) %>%
select(Unit, min_dist_m) %>%
mutate(min_dist_miles = round(min_dist_m*0.0006213712)) #convert meters to miles
#Add everything to the df object
df_add = df %>%
left_join(y = pts_in_poly, by = "Unit") %>%
inner_join(y = dist_t, by = "Unit") %>%
mutate(Known_Presences = ifelse(is.na(Known_Presences), 0, Known_Presences),
perc_suit = ifelse(perc_suit > 100, 1, perc_suit/100),
est_suit_area_acre = ifelse(perc_suit == 100, total_area_acre, est_suit_area_acre),
Admin = unique(shp.n$Admin)) %>%
select(Species, Admin, Unit, est_suit_area_acre, total_area_acre, perc_suit, Known_Presences, min_dist_miles) %>%
arrange(desc(perc_suit))
df.list[[paste0(unique(df_add$Admin))]] <- df_add
}
for(s in shp.list[3]){
# Here, the ensemble_1st from gdal_calc of 1 where A >= n and NoDataValue=0
shp.t <- readOGR(s, stringsAsFactors = F, GDAL1_integer64_policy = T, verbose = F)
# Transform shp to raster's crs
shp.n = spTransform(shp.t, CRSobj = crs(r.l))
# shp.n = shp.n[1:10,]
#Pull zonal summary of threshold raster - this is the part that takes the longest
result = zonal.stats(x = shp.n, y = r.l, stat = sum) #results match zonal stats from QGIS
result = ifelse(is.na(result), 0, result)
# convert results into data frame. readOGR orders things in a weird way but this method syncs everything up
df = data.frame(Species = species.name
, Unit = shp.n$Unit
, count_n = result)
# Determine rasters size and add square km area to data frame (assumes projection is using meters)
df = as.data.frame(df) %>%
mutate(est_suit_area_km2 = (count_n*(ps.sq/1000000)),
total_area_km2 = area(shp.n)/1000000,
est_suit_area_acre = round(est_suit_area_km2*247.11),
total_area_acre = round(total_area_km2*247.11),
perc_suit = round(est_suit_area_km2/total_area_km2*100))
# Figure out which parks already have known occurrences. Then count them and add them to the 'df' object
pts_in_poly <- spatialEco::point.in.poly(x = sp.df, y = shp.n, duplicate = F)
pts_in_poly <- pts_in_poly@data[!is.na(pts_in_poly@data$pid1),]
pts_in_poly <- pts_in_poly %>%
group_by(pid1) %>%
count() %>%
mutate(Known_Presences = paste0("Yes (", n, ")", sep='')) %>%
rename(Unit = pid1) %>%
droplevels
# Determine the distance from polygon vertices to nearest occurrence point (not a perfect solution but pretty speedy)
dist = as.data.frame(gDistance(spgeom1 = shp.n, spgeom2 = sp.df, byid = T))
colnames(dist) <- shp.n@data$Unit
dist_t = as.data.frame(apply(dist, 2, min)) %>%
mutate(Unit = names(dist)) %>%
rename(min_dist_m = 1) %>%
select(Unit, min_dist_m) %>%
mutate(min_dist_miles = round(min_dist_m*0.0006213712)) #convert meters to miles
#Add everything to the df object
df_add = df %>%
left_join(y = pts_in_poly, by = "Unit") %>%
inner_join(y = dist_t, by = "Unit") %>%
mutate(Known_Presences = ifelse(is.na(Known_Presences), 0, Known_Presences),
perc_suit = ifelse(perc_suit > 100, 1, perc_suit/100),
est_suit_area_acre = ifelse(perc_suit == 100, total_area_acre, est_suit_area_acre),
Admin = unique(shp.n$Admin)) %>%
select(Species, Admin, Unit, est_suit_area_acre, total_area_acre, perc_suit, Known_Presences, min_dist_miles) %>%
arrange(desc(perc_suit))
df.list[[paste0(unique(df_add$Admin))]] <- df_add
}
# Flatten the data frames and write them to the outfile.
df_final = bind_rows(df.list)
# Keeping checking the spatialEco library to see if Jeff integrated my changes into the CRAN version.
zonal.stats <- function(x, y, stat, trace = TRUE) {
if (class(y) != "RasterLayer")
stop("y must be a raster object")
if (class(x) != "SpatialPolygonsDataFrame")
stop("x must be a SpatialPolygonsDataFrame object")
results <- vector()
for (j in 1:nrow(x)) {
if (trace == TRUE) {
cat("Processing", j, "of", nrow(x), "\n")
}
lsub <- sf::st_as_sf(x[j, ])
cr <- raster::crop(y, raster::extent(x[j,]), snap = "out")
crop.NA <- raster::setValues(cr, NA)
fr <- fasterize::fasterize(lsub, cr)
r <- raster::mask(x = cr, mask = fr)
r <- raster::values(r)
r <- stats::na.omit(r)
if (length(r) < 1) {
results <- append(results, NA)
} else {
results <- append(results, stat(r))
}
}
results
}
summary.stats = function(py.path, gdal.calc.path, in.rast, max.val, species.name, pts, outfile){
# Create threshold raster
out.rast = paste0(tools::file_path_sans_ext(in.rast), '_gte.tif', sep = '')
command = paste0(shQuote(py.path), ' ', shQuote(gdal.calc.path),
' -A ', shQuote(in.rast),
' --type=Int16 --calc="1*(logical_and(A >= ',ceiling(max.val/2),
', A < 100))" --co=COMPRESS=LZW --co=TILED=YES --NoDataValue=0.0 --overwrite --outfile=',
shQuote(out.rast),
sep = '')
system(command)
# Since this is hard-coded, don't move this folder!
# shp.list <- list.files('//igskbacbfs1/InvasivesSynologyData/Projects/NPS/PostProcessingScripts/zonal_stats_shapefiles',
#                        pattern = '(.*zonal.*shp$)',  recursive = T, full.names = T)
shp.list <- list.files('E:/Users/engelstad/USGS/data/zonal stats shapefiles',
pattern = '(.*zonal.*shp$)',  recursive = T, full.names = T)
# Generate raster metadata.
r.l <- raster(out.rast)
r.i <- GDALinfo(out.rast, silent = T)
ps.sq = r.i[6]*r.i[7]
# Read in MDS file to use for point generation.
csv = read.csv(pts, header = T, stringsAsFactors = F)
csv = csv[3:nrow(csv), 1:3]
csv = as.data.frame(sapply(csv[csv$responseBinary ==1, ], as.numeric))
# This line assumes that the input raster and the points are in the same projection.
sp.df = SpatialPointsDataFrame(coords = csv, data = csv, proj4string = crs(r.l))
# This variable will house the df objects from each successive shapefile in the loop
df.list <- list()
for(s in shp.list[3]){
# Here, the ensemble_1st from gdal_calc of 1 where A >= n and NoDataValue=0
shp.t <- readOGR(s, stringsAsFactors = F, GDAL1_integer64_policy = T, verbose = F)
# Transform shp to raster's crs
shp.n = spTransform(shp.t, CRSobj = crs(r.l))
# shp.n = shp.n[1:10,]
#Pull zonal summary of threshold raster - this is the part that takes the longest
result = zonal.stats(x = shp.n, y = r.l, stat = sum) #results match zonal stats from QGIS
result = ifelse(is.na(result), 0, result)
# convert results into data frame. readOGR orders things in a weird way but this method syncs everything up
df = data.frame(Species = species.name
, Unit = shp.n$Unit
, count_n = result)
# Determine rasters size and add square km area to data frame (assumes projection is using meters)
df = as.data.frame(df) %>%
mutate(est_suit_area_km2 = (count_n*(ps.sq/1000000)),
total_area_km2 = area(shp.n)/1000000,
est_suit_area_acre = round(est_suit_area_km2*247.11),
total_area_acre = round(total_area_km2*247.11),
perc_suit = round(est_suit_area_km2/total_area_km2*100))
# Figure out which parks already have known occurrences. Then count them and add them to the 'df' object
pts_in_poly <- spatialEco::point.in.poly(x = sp.df, y = shp.n, duplicate = F)
pts_in_poly <- pts_in_poly@data[!is.na(pts_in_poly@data$pid1),]
pts_in_poly <- pts_in_poly %>%
group_by(pid1) %>%
count() %>%
mutate(Known_Presences = paste0("Yes (", n, ")", sep='')) %>%
rename(Unit = pid1) %>%
droplevels
# Determine the distance from polygon vertices to nearest occurrence point (not a perfect solution but pretty speedy)
dist = as.data.frame(gDistance(spgeom1 = shp.n, spgeom2 = sp.df, byid = T))
colnames(dist) <- shp.n@data$Unit
dist_t = as.data.frame(apply(dist, 2, min)) %>%
mutate(Unit = names(dist)) %>%
rename(min_dist_m = 1) %>%
select(Unit, min_dist_m) %>%
mutate(min_dist_miles = round(min_dist_m*0.0006213712)) #convert meters to miles
#Add everything to the df object
df_add = df %>%
left_join(y = pts_in_poly, by = "Unit") %>%
inner_join(y = dist_t, by = "Unit") %>%
mutate(Known_Presences = ifelse(is.na(Known_Presences), 0, Known_Presences),
perc_suit = ifelse(perc_suit > 100, 1, perc_suit/100),
est_suit_area_acre = ifelse(perc_suit == 100, total_area_acre, est_suit_area_acre),
Admin = unique(shp.n$Admin)) %>%
select(Species, Admin, Unit, est_suit_area_acre, total_area_acre, perc_suit, Known_Presences, min_dist_miles) %>%
arrange(desc(perc_suit))
df.list[[paste0(unique(df_add$Admin))]] <- df_add
}
# Flatten the data frames and write them to the outfile.
df_final = bind_rows(df.list)
write.csv(df_final, file = outfile)
}
# py.path = "C:/Program Files/QGIS 2.18/bin/python.exe"
py.path = "E:/Users/engelstad/VisTrails_SAHM/Python27_64/python.exe"
gdal.calc.path = "E:/Users/engelstad/VisTrails_SAHM/Python27_64/Lib/site-packages/osgeo/gdal_calc.py"
in.rast = "E:/Users/engelstad/USGS/data/zonal stats shapefiles/ensemble_1st.tif"
max.val = 3
pts = "E:/Users/engelstad/USGS/data/20190723/fountaingrass/CovariateCorrelationOutputMDS_KDE_initial.csv"
species.name = "fountaingrass"
outfile = "E:/Users/engelstad/USGS/data/zonal stats shapefiles/test.csv"
summary.stats(py.path = py.path,
gdal.calc.path = gdal.calc.path,
in.rast = in.rast,
max.val = max.val,
species.name = species.name,
pts = pts,
outfile = outfile)
summary.stats = function(py.path, gdal.calc.path, in.rast, max.val, species.name, pts, outfile){
# Create threshold raster
out.rast = paste0(tools::file_path_sans_ext(in.rast), '_gte.tif', sep = '')
command = paste0(shQuote(py.path), ' ', shQuote(gdal.calc.path),
' -A ', shQuote(in.rast),
' --type=Int16 --calc="1*(logical_and(A >= ',ceiling(max.val/2),
', A < 100))" --co=COMPRESS=LZW --co=TILED=YES --NoDataValue=0.0 --overwrite --outfile=',
shQuote(out.rast),
sep = '')
system(command)
# Since this is hard-coded, don't move this folder!
# shp.list <- list.files('//igskbacbfs1/InvasivesSynologyData/Projects/NPS/PostProcessingScripts/zonal_stats_shapefiles',
#                        pattern = '(.*zonal.*shp$)',  recursive = T, full.names = T)
shp.list <- list.files('E:/Users/engelstad/USGS/data/zonal stats shapefiles',
pattern = '(.*zonal.*shp$)',  recursive = T, full.names = T)
# Generate raster metadata.
r.l <- raster(out.rast)
r.i <- GDALinfo(out.rast, silent = T)
ps.sq = r.i[6]*r.i[7]
# Read in MDS file to use for point generation.
csv = read.csv(pts, header = T, stringsAsFactors = F)
csv = csv[3:nrow(csv), 1:3]
csv = as.data.frame(sapply(csv[csv$responseBinary ==1, ], as.numeric))
# This line assumes that the input raster and the points are in the same projection.
sp.df = SpatialPointsDataFrame(coords = csv, data = csv, proj4string = crs(r.l))
# This variable will house the df objects from each successive shapefile in the loop
df.list <- list()
for(s in shp.list){
# Here, the ensemble_1st from gdal_calc of 1 where A >= n and NoDataValue=0
shp.t <- readOGR(s, stringsAsFactors = F, GDAL1_integer64_policy = T, verbose = F)
# Transform shp to raster's crs
shp.n = spTransform(shp.t, CRSobj = crs(r.l))
# shp.n = shp.n[1:10,]
#Pull zonal summary of threshold raster - this is the part that takes the longest
result = zonal.stats(x = shp.n, y = r.l, stat = sum) #results match zonal stats from QGIS
result = ifelse(is.na(result), 0, result)
# convert results into data frame. readOGR orders things in a weird way but this method syncs everything up
df = data.frame(Species = species.name
, Unit = shp.n$Unit
, count_n = result)
# Determine rasters size and add square km area to data frame (assumes projection is using meters)
df = as.data.frame(df) %>%
mutate(est_suit_area_km2 = (count_n*(ps.sq/1000000)),
total_area_km2 = area(shp.n)/1000000,
est_suit_area_acre = round(est_suit_area_km2*247.11),
total_area_acre = round(total_area_km2*247.11),
perc_suit = round(est_suit_area_km2/total_area_km2*100))
# Figure out which parks already have known occurrences. Then count them and add them to the 'df' object
pts_in_poly <- spatialEco::point.in.poly(x = sp.df, y = shp.n, duplicate = F)
pts_in_poly <- pts_in_poly@data[!is.na(pts_in_poly@data$pid1),]
pts_in_poly <- pts_in_poly %>%
group_by(pid1) %>%
count() %>%
mutate(Known_Presences = paste0("Yes (", n, ")", sep='')) %>%
rename(Unit = pid1) %>%
droplevels
# Determine the distance from polygon vertices to nearest occurrence point (not a perfect solution but pretty speedy)
dist = as.data.frame(gDistance(spgeom1 = shp.n, spgeom2 = sp.df, byid = T))
colnames(dist) <- shp.n@data$Unit
dist_t = as.data.frame(apply(dist, 2, min)) %>%
mutate(Unit = names(dist)) %>%
rename(min_dist_m = 1) %>%
select(Unit, min_dist_m) %>%
mutate(min_dist_miles = round(min_dist_m*0.0006213712)) #convert meters to miles
#Add everything to the df object
df_add = df %>%
left_join(y = pts_in_poly, by = "Unit") %>%
inner_join(y = dist_t, by = "Unit") %>%
mutate(Known_Presences = ifelse(is.na(Known_Presences), 0, Known_Presences),
perc_suit = ifelse(perc_suit > 100, 1, perc_suit/100),
est_suit_area_acre = ifelse(perc_suit == 100, total_area_acre, est_suit_area_acre),
Admin = unique(shp.n$Admin)) %>%
select(Species, Admin, Unit, est_suit_area_acre, total_area_acre, perc_suit, Known_Presences, min_dist_miles) %>%
arrange(desc(perc_suit))
df.list[[paste0(unique(df_add$Admin))]] <- df_add
}
# Flatten the data frames and write them to the outfile.
df_final = bind_rows(df.list)
write.csv(df_final, file = outfile)
}
# py.path = "C:/Program Files/QGIS 2.18/bin/python.exe"
py.path = "E:/Users/engelstad/VisTrails_SAHM/Python27_64/python.exe"
gdal.calc.path = "E:/Users/engelstad/VisTrails_SAHM/Python27_64/Lib/site-packages/osgeo/gdal_calc.py"
in.rast = "E:/Users/engelstad/USGS/data/zonal stats shapefiles/ensemble_1st.tif"
max.val = 3
pts = "E:/Users/engelstad/USGS/data/20190723/fountaingrass/CovariateCorrelationOutputMDS_KDE_initial.csv"
species.name = "fountaingrass"
outfile = "E:/Users/engelstad/USGS/data/zonal stats shapefiles/test.csv"
summary.stats(py.path = py.path,
gdal.calc.path = gdal.calc.path,
in.rast = in.rast,
max.val = max.val,
species.name = species.name,
pts = pts,
outfile = outfile)
options(rsconnect.check.certificate = FALSE)
rsconnect::setAccountInfo(name='engelstad',
token='B4DF26C4AD47A8262A1DCB8231824B7D',
secret='+NvOBoYjgvnwSvuCa9TlT6hHkKcugNq6FZt/o58T')
setwd("E:/Users/engelstad/GitHub/inhabit_sandbox/")
getwd()
# setwd("E:/Users/engelstad/USGS/dashboard/")
#due to CSU firewall, this next line needs to be run
rsconnect::deployApp(account = "engelstad",appName = "sandbox", appPrimaryDoc='sandbox.rmd', appSourceDoc = 'sandbox.rmd')
options(rsconnect.check.certificate = FALSE)
rsconnect::setAccountInfo(name='engelstad',
token='B4DF26C4AD47A8262A1DCB8231824B7D',
secret='+NvOBoYjgvnwSvuCa9TlT6hHkKcugNq6FZt/o58T')
setwd("E:/Users/engelstad/GitHub/inhabit_sandbox/")
getwd()
# setwd("E:/Users/engelstad/USGS/dashboard/")
#due to CSU firewall, this next line needs to be run
rsconnect::deployApp(account = "engelstad",appName = "sandbox", appPrimaryDoc='sandbox.rmd', appSourceDoc = 'sandbox.rmd')
.libPaths( c( "C:/LocalData/Source/RPackages",.libPaths()))
install.packages("formattable")
